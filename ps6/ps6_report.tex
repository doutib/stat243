\documentclass{llncs}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{listings}
\usepackage{moreverb}
\usepackage{inconsolata}
\pagestyle{plain}


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\title{Problem Set 6}
\author{Thibault Doutre, Student ID 26980469}
\institute{STAT243 : Statistical Computing\\
University of California, Berkeley}
\date{\today}
\maketitle
\bigbreak
\noindent
I worked on my own.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{SQL}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Airline Database}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
I create a script which I can execute via EC2. The script builds a database "airline.db" and store every file into a single table, using RSQLite. In order to do it, I first download the files in a directory named "data" and create the database. In this database, I store a table called "airline" in wich I will append the data for every year.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{#Download}
\hlstd{url} \hlkwb{=} \hlstr{'http://www.stat.berkeley.edu/share/paciorek/1987-2008.csvs.tgz'}
\hlkwd{download.file}\hlstd{(url,} \hlstr{".file"}\hlstd{)}

\hlcom{# Untar}
\hlkwd{untar}\hlstd{(}\hlstr{".file"}\hlstd{,} \hlkwc{compressed} \hlstd{=} \hlstr{'bzip2'}\hlstd{,} \hlkwc{exdir} \hlstd{=} \hlstr{"./data/"}\hlstd{)}

\hlcom{# Create Database}
\hlkwd{library}\hlstd{(RSQLite)}
\hlstd{drv} \hlkwb{<-} \hlkwd{dbDriver}\hlstd{(}\hlstr{"SQLite"}\hlstd{)}
\hlstd{db} \hlkwb{<-} \hlkwd{dbConnect}\hlstd{(drv,} \hlkwc{dbname} \hlstd{=} \hlstr{"airline.db"}\hlstd{)}

\hlcom{# Create airline Table}
\hlkwd{dbSendQuery}\hlstd{(}\hlkwc{conn} \hlstd{= db,}
            \hlstr{[1333 chars quoted with '"']}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
Then, for every year:
\begin{itemize}
\item Open a connection to the file with bzcat 
\item Get the data and store it into a variable "line"
\item Create a temporary table from line
\item Append this table to "airline" using INSERT
\item Remove the temporary table
\item Close the connection
\end{itemize}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Append years into airline table}
\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1987}\hlopt{:}\hlnum{2008}\hlstd{)\{}
  \hlstd{con}\hlkwb{=}\hlkwd{pipe}\hlstd{(}\hlkwd{paste}\hlstd{(}\hlstr{"bzcat "}\hlstd{,i,}\hlstr{".csv.bz2"}\hlstd{,}\hlkwc{sep}\hlstd{=}\hlstr{""}\hlstd{),} \hlkwc{open} \hlstd{=} \hlstr{'r'}\hlstd{)}
  \hlstd{lines} \hlkwb{=} \hlkwd{read.csv}\hlstd{(con,} \hlkwc{header} \hlstd{=} \hlnum{TRUE}\hlstd{)}
  \hlkwd{dbWriteTable}\hlstd{(db,} \hlkwd{paste}\hlstd{(}\hlstr{"y"}\hlstd{,i,}\hlkwc{sep}\hlstd{=}\hlstr{""}\hlstd{),lines)}
  \hlkwd{dbSendQuery}\hlstd{(db,}\hlkwd{paste}\hlstd{(}\hlstr{"INSERT INTO airline SELECT * FROM y"}\hlstd{,i,}\hlkwc{sep}\hlstd{=}\hlstr{""}\hlstd{))}
  \hlkwd{dbRemoveTable}\hlstd{(db,}\hlkwd{paste}\hlstd{(}\hlstr{"y"}\hlstd{,i,}\hlkwc{sep}\hlstd{=}\hlstr{""}\hlstd{))}
  \hlkwd{close}\hlstd{(con)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
Then, I print the total numer of lines in the database:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Size in Gb}
\hlkwd{file.size}\hlstd{(}\hlstr{"./airline.db"}\hlstd{)}\hlopt{/}\hlnum{2}\hlopt{^}\hlnum{30}
\hlcom{# 9.273604}

\hlcom{# Number of lines}
\hlkwd{dbGetQuery}\hlstd{(db,} \hlstr{"SELECT COUNT(*) FROM airline"}\hlstd{)}
\hlcom{# 123534969}
\end{alltt}
\end{kframe}
\end{knitrout}
We can see that the database is 9 Gb big, it is less than the original CSV of 12 Gb but significantly bigger than the bzipped file of 1.7 Gb. We can see that the airline table in the database has one hundred million rows a mere.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Aggregating}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, I aggregate information into categories specified by the problem set. In order to do it, I write a Rscript which I can run using EC2. In the script, I first load the database.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Load Database}
\hlkwd{print}\hlstd{(}\hlstr{'Loading db...'}\hlstd{)}
\hlkwd{library}\hlstd{(RSQLite)}
\hlstd{fileName}\hlkwb{=}\hlstr{"airline.db"}
\hlstd{drv} \hlkwb{=} \hlkwd{dbDriver}\hlstd{(}\hlstr{"SQLite"}\hlstd{)}
\hlstd{db} \hlkwb{=} \hlkwd{dbConnect}\hlstd{(drv,} \hlkwc{dbname} \hlstd{= fileName)}
\end{alltt}
\end{kframe}
\end{knitrout}
Then, I create a new table, which is in fact a subset of the airline dataset, when eliminating missing values for DepDelay and outliers. I chose to eliminate the same points as in unit7 i.e. departure delays which are less than 720 and more than -30.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Create a new table from subsetting airline}
\hlkwd{print}\hlstd{(}\hlstr{'Creating a subset of airline...'}\hlstd{)}
\hlkwd{dbSendQuery}\hlstd{(db,} \hlstr{"CREATE TABLE airline1 AS 
            SELECT *
            FROM   airline
            WHERE  DepDelay > -30
            AND DepDelay < 720
            AND DepDelay != 'NA'"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
Now, I send a querry, according to the threshold we want for DepDelay (30, 60 and 180). I use GROUP BY method over the selected columns. I also mapped the number of month with the name of the month and the number of day in the week with the corresponding names of them, using substr. Moreover, I display the hour of the CRSTime Departure by rounding the CRSDepTime value. Finally, I use count and coalesce functions in order to extract the percentage of Delay above the specified threshold. I also print the count of DepDelay per category, because I think that it is relevant to our statistics and it will be usefull for the next step. \\
I could have also done three different querries for the three thresholds but it would have been much slower.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Create required table with percentage of DepDelays}
\hlkwd{print}\hlstd{(}\hlstr{'Aggregating table...'}\hlstd{)}
\hlkwd{system.time}\hlstd{(}
\hlkwd{dbSendQuery}\hlstd{(db,}\hlstr{"CREATE TABLE Delay30 AS
  SELECT 
           Origin, 
           Dest,
           substr('Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ', 
(Month * 4) - 3, 3) AS Month,
           substr('Mon Tue Wed Thu Fri Sat Sun ', (DayOfWeek * 4) - 3, 3) 
AS Day,
           ROUND(CRSDepTime/100) AS Hour,
           coalesce(round(count(case when DepDelay > 180 then 1 end)/
(count(DepDelay)+.0)*100), 0) as PercentageDelay,
           coalesce(round(count(case when DepDelay > 60 then 1 end)/
(count(DepDelay)+.0)*100), 0) as PercentageDelay,
           coalesce(round(count(case when DepDelay > 30 then 1 end)/
(count(DepDelay)+.0)*100), 0) as PercentageDelay,
           count(DepDelay) AS count
           FROM airline1 
           GROUP BY Origin, Dest, Month, Day, Hour"}\hlstd{)}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
The output looks like this:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
Origin Dest Month Day Hour PercentageDelay180 PercentageDelay60 PercentageDelay30
1    ABE  ATL   Jan Fri    7                  0                 0                25
2    ABE  ATL   Jan Fri   13                  0                 0                 0
3    ABE  ATL   Jan Fri   18                  0                 0                 0
4    ABE  ATL   Jan Mon    7                  0                 0                 0
5    ABE  ATL   Jan Mon   13                  0                20                20
6    ABE  ATL   Jan Mon   18                  0                 0                 0
\end{alltt}
\end{kframe}
\end{knitrout}
I have the following complexity results for aggregating values:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
   user  system elapsed 
560.296  42.220 683.313 
\end{alltt}
\end{kframe}
\end{knitrout}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Indexing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Let's compare the speed of aggregating when indexing the variables. I create a new script, which creates 3 new tables in the exact same way than previously but with indexing the subsetted airline1 table. I index the table this way:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Create index}
\hlkwd{dbSendQuery}\hlstd{(db,}\hlstr{"CREATE INDEX index_name ON 
            airline1 (Origin,Dest,Month,DayOfWeek,CRSDepTime,DepDelay)"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
I got the following complexity results:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
   user  system elapsed 
410.720  36.892 505.591
\end{alltt}
\end{kframe}
\end{knitrout}
The results are significantly better when indexing the tables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Top results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to have the top 10 groupings keys in terms of proportion of late flights for groupings with at least 150 flights, I send this query, using the previous tables with the count field:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{dbGetQuery}(db,"SELECT * FROM Delay180I 
           \hlkwd{WHERE} (count >150) 
           ORDER BY PercentageDelay DESC 
           LIMIT 10 ")
\hlcom{# Result}
   Origin Dest Month Day Hour PercentageDelay count
1     EWR  ORD   Jun Wed   17              10   167
2     EWR  LAX   Jun Wed   17               8   170
3     EWR  ORD   Jun Wed   18               8   165
4     EWR  ORD   Jul Fri   17               8   165
5     EWR  ORD   Jul Fri   18               8   161
6     EWR  ORD   Jul Wed   18               8   171
7     EWR  ORD   Jun Fri   17               7   175
8     EWR  ORD   Jun Wed   16               7   196
9     EWR  ORD   Jun Wed   19               7   151
10    EWR  ORD   Aug Fri   18               7   175
\end{alltt}
\end{kframe}
\end{knitrout}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallel Processing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In order to parallel the three queries we can use a foreach loop. In this loop I compute each of the querries and aggregate them using the .combine option. I artificially add the first columns Origin, Dest, Month, Day, Hour in another parallelized task. Also, we have to be aware that the database is already indexed.\\
In fact, the real time spent to compute all the tasks should be approximately three times less than computing the three querries in a row. However, and it is very important to notice, calculating the three ratios in the same querry should be as fast as the parallelized code because SQL may have a smart way to compute the operations since they are asked in the same querry.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(foreach)}
\hlkwd{library}\hlstd{(doMC)}

\hlcom{# Set up 4 cores }
\hlkwd{registerDoMC}\hlstd{(}\hlkwc{cores} \hlstd{=} \hlnum{4}\hlstd{)}

\hlkwd{system.time}\hlstd{(\{}
  \hlkwd{print}\hlstd{(}\hlstr{'computing aggregation...'}\hlstd{)}
  \hlstd{out}\hlkwb{=}\hlkwd{foreach}\hlstd{(}\hlkwc{i} \hlstd{=} \hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{1}\hlstd{,}\hlnum{180}\hlstd{,}\hlnum{60}\hlstd{,}\hlnum{30}\hlstd{),} \hlkwc{.combine} \hlstd{= c)} \hlopt{%dopar%} \hlstd{\{}
    \hlkwa{if} \hlstd{(i}\hlopt{==-}\hlnum{1}\hlstd{)\{}
      \hlstd{db0}\hlkwb{=}\hlkwd{dbConnect}\hlstd{(drv,} \hlkwc{dbname} \hlstd{= fileName)}
      \hlstd{s}\hlkwb{=}\hlkwd{dbGetQuery}\hlstd{(db0,}\hlstr{" SELECT 
                   Origin, 
                   Dest,
                   substr('Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec '
                   ,(Month * 4) - 3, 3) AS Month,
                   substr('Mon Tue Wed Thu Fri Sat Sun ', (DayOfWeek * 4) - 
                   3, 3) AS Day,
                   ROUND(CRSDepTime/100) AS Hour
                   FROM airline1 
                   GROUP BY Origin, Dest, Month, Day, Hour"}\hlstd{)}
      \hlkwd{return}\hlstd{(s)}
      \hlkwd{dbDisconnect}\hlstd{(db0)}
    \hlstd{\}}
    \hlkwa{else}\hlstd{\{}
      \hlstd{db1}\hlkwb{=}\hlkwd{dbConnect}\hlstd{(drv,} \hlkwc{dbname} \hlstd{= fileName)}
      \hlstd{s}\hlkwb{=}\hlkwd{dbGetQuery}\hlstd{(db1,}\hlkwd{paste}\hlstd{(}\hlstr{"
                             SELECT 
                             coalesce(round(count(case when DepDelay >"}\hlstd{,i,}
                             \hlstr{" then 1 end)/(count(DepDelay)+.0)*100), 0) as 
                             PercentageDelay"}\hlstd{,i,}\hlstr{"
                             FROM airline1 
                             GROUP BY Origin, Dest, Month, DayOfWeek, 
                             ROUND(CRSDepTime/100)"}\hlstd{,}\hlkwc{sep}\hlstd{=}\hlstr{""}\hlstd{))}
      \hlkwd{return}\hlstd{(s)}
      \hlkwd{dbDisconnect}\hlstd{(db1)}
    \hlstd{\}}
  \hlstd{\}}
  \hlkwd{print}\hlstd{(}\hlstr{'writing table...'}\hlstd{)}
  \hlkwd{dbWriteTable}\hlstd{(db,}\hlstr{"summary"}\hlstd{,}\hlkwd{as.data.frame}\hlstd{(out))}
\hlstd{\})}
\end{alltt}
\end{kframe}
\end{knitrout}
We indeed see that the task is parallelized by noticing that the user time is higher than the elapsed time. However, as mentionned above, the elapsed time is similar tothe elapsed time spent with one query. Here are my results on a m3xlarge instance on the whole data:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
    user   system  elapsed 
1543.842  227.148  773.935 
\end{alltt}
\end{kframe}
\end{knitrout}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Spark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using spark, I compute the following chunk of code after connecting myself to ec2. Basically the commands before entering spark are:
\begin{itemize}
\item Log into EC2
\begin{lstlisting}[frame=single] 
export SPARK_VERSION=1.5.1
export CLUSTER_SIZE=12  # number of slave nodes
export mycluster=sparkvm-thibault.doutre # need unique
name relative to other users

cd /usr/local/lib/spark/ec2/

export AWS_ACCESS_KEY_ID=`grep aws_access_key_id stat
243-fall-2015-credentials.boto | cut -d' ' -f3`
export AWS_SECRET_ACCESS_KEY=`grep aws_secret_access
_key stat243-fall-2015-credentials.boto | cut -d' ' -f3`

######### NB CLUSTERS ###########
export NUMBER_OF_WORKERS=12

######### 10 minutes
./spark-ec2 -k thibault.doutre@berkeley.edu:stat243-
fall-2015 -i ~/.ssh/stat243-fall-2015-ssh_key.pem 
--region=us-west-2 -s ${NUMBER_OF_WORKERS} -v 1.5.1 
launch sparkvm-thibault.doutre

######### login
./spark-ec2 -k thibault.doutre@berkeley.edu:stat243-fall
-2015 -i ~/.ssh/stat243-fall-2015-ssh_key.pem --region=
us-west-2 login sparkvm-thibault.doutre
\end{lstlisting}
\item Get the files in the right folder
\begin{lstlisting}[frame=single] 
export PATH=$PATH:/root/ephemeral-hdfs/bin/

hadoop fs -mkdir /data
hadoop fs -mkdir /data/airline

df -h
mkdir /mnt/airline
cd /mnt/airline

wget http://www.stat.berkeley.edu/share/paciorek/1987
-2008.csvs.tgz
tar -xvzf 1987-2008.csvs.tgz

hadoop fs -copyFromLocal /mnt/airline/*bz2 /data/airline
\end{lstlisting}
\item Log into spark
\begin{lstlisting}[frame=single] 
# python 2.7
yum install -y python27-pip python27-devel
# numpy
pip-2.7 install 'numpy==1.9.2'  
# pyspark is in /root/spark/bin
export PATH=${PATH}:/root/spark/bin
# start Spark's Python interface as interactive session
pyspark
\end{lstlisting}
\end{itemize}
Then comes the spark code. These are the steps:
\begin{itemize}
\item Filter out the NA values and the outliers
\item Repartition
\item Mapping
\item Reducing
\end{itemize}
The screen function is used to filter data and the stratify function to map the three values we want: 
\begin{itemize}
\item cnt1 corresponds to the 30 threshold 
\item cnt2 corresponds to the 60 threshold
\item cnt3 corresponds to the 180 threshold
\end{itemize}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
from operator import add
import numpy as np

lines = \hlkwd{sc.textFile}(\hlstr{'/data/airline'})

def \hlkwd{screen}(vals):
    vals = \hlkwd{vals.split}(\hlstr{','})
    \hlkwd{return}(vals[0] != \hlstr{'Year'} and vals[15] != \hlstr{'NA'} and \hlkwd{float}(vals[15]) 
           < 720 and \hlkwd{float}(vals[15]) > -30 )


def \hlkwd{stratify}(line):
    vals = \hlkwd{line.split}(\hlstr{','})
    if vals[0] == \hlstr{'Year'}:
        \hlkwd{return}(\hlstr{'0'}, [0,0,0])
    else:
        keyVals = \hlstr{'-'}\hlkwd{.join}([vals[1],vals[3],\hlkwd{str}(\hlkwd{int}(\hlkwd{float}(vals[5])/100))
                            ,vals[16],vals[17]])
        v=\hlkwd{int}(vals[15])
        cnt1=0
        cnt2=0
        cnt3=0
        if v>30:
            cnt1=1   
            if v>60: 
                cnt2=1 
                if v>180:
                    cnt3=1 
        \hlkwd{return}(keyVals, [cnt1,cnt2,cnt3])



output = \hlkwd{lines.filter}(screen)\hlkwd{.repartition}(48)\hlkwd{.map}(stratify).
\hlkwd{reduceByKey}(add)
result = \hlkwd{output.collect}()
\end{alltt}
\end{kframe}
\end{knitrout}
Then, in order to put the result, which is a list into a text file, I do another mapping and save the result with saveAsTextFile.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
def \hlkwd{stratify2}(elt):
    keyVals = \hlstr{','}\hlkwd{.join}(\hlkwd{map}(str,elt))
    \hlkwd{return}(keyVals)

\hlkwd{result.map}(stratify2)\hlkwd{.saveAsTextFile}(\hlstr{'/data/summary'})
\end{alltt}
\end{kframe}
\end{knitrout}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preprocessing using Bash}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Since we only use a subset of the data, we can eliminate useless columns using bash. And we know that this is really fast. I will write a script which actually does this. First, I create a new folder where I can put the files.
\begin{lstlisting}[frame=single] 
mkdir dataSubset
\end{lstlisting}
Then, I run the following script which process data using bzcat and save it to a bz2 file, for every year. I store the new data in dataSubset.
\begin{lstlisting}[frame=single] 
# Create script
vim Script_subset.sh
\end{lstlisting}
\begin{lstlisting}[frame=single] 
#!/bin/bash

subset_data()
{
for i in {1987..2008}
do
        echo $i
        bzcat data/$i.csv.bz2 | cut -d',' -f2,4,6,16,17,18 |  bzip2 > dataSubset/$i$subset.csv.bz2
done
}
\end{lstlisting}
Now I can execute the script:
\begin{lstlisting}[frame=single] 
source Script_subset.sh ; subset_data
\end{lstlisting}
The process takes approximately 10 minutes (9min50s) to run on a m3.xlarge instance. Normally, to create the SQL database, it takes 50 minutes approximately. But with this preprocessing it takes 10 minutes, which is 20 minutes total approximately. Therefore, it is better to use preprocessing in bash.


\end{document}









